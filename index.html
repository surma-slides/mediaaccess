<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Media Access</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/mods.css">
		<link rel="stylesheet" href="css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Media Access</h1>
          <p>
            <small>by <a href="http://twitter.com/surmair" target="_blank">Alexander “Surma” Surma</a>, October 2014</small>
          </p>
					<p>
						<small><a href="http://surma-slides.github.io/mediaaccess">surma-slides.github.io/mediaaccess</a></small>
					</p>
				</section>

				<section>
					<h2>Included you will find…</h2>
					<p>
						Bundle of standards (mostly drafts) about different kinds of access to different kinds of media.
            <video src="assets/airbnb.webm" autoplay="autoplay" loop></video>
					</p>

					<aside class="notes">
            Media is video, audio, webpages, anything.
						2 completed standards (+ Ontology), 1 working candidate, 8 drafts.
						Use-cases et al. not included.
            Different sizes, extensions and a lot of overlap.
					</aside>
				</section>
				<section>
					<section data-state="subtheme-indigo">
						<h2>Buzzwords!</h2>
						<p>
							Who has <em>not</em> heared of <code>getUserMedia()</code> or <code>Web Audio API</code>?
						</p>
						<aside class="notes">
							Arguably the most hyped and well known standards out of the media access collection.
						</aside>
					</section>
          <section data-state="subtheme-indigo">
            <h2>Buzzwords!</h2>
            <iframe width="560" height="315" src="//www.youtube.com/embed/MsAWR_rJ5n8" frameborder="0" allowfullscreen></iframe>
            <p>
              February 3, 2013
            </p>
            <aside class="notes">
              WebRTC = Web Real-Time Communication. A lot of synergy and mutual motivation.
            </aside>
          </section>
          <section data-state="subtheme-indigo">
            <h2>Buzzwords!</h2>
            <iframe width="560" height="315" src="//www.youtube.com/embed/cqtBpCqgOgM?start=400" frameborder="0" allowfullscreen></iframe>
            <p>
              October 9, 2014
            </p>
            <aside class="notes">
              Jan Moschke. Extensive User of WebAPI. Collaborative music editor with WebRTC. DevFest 2013 & 2014.
            </aside>
          </section>
					<section data-state="subtheme-indigo">
						<h2><code>getUserMedia()</code></h2>
						<p>
							<a class="usermedia demo">Demo</a>
						</p>
						<video class="usermedia demo" width="400" height="300"></video>
						<script>
							document.querySelector('a.usermedia.demo').addEventListener('click', function(){
								navigator.getUserMedia = (navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia);
								if (navigator.getUserMedia) {
									navigator.getUserMedia({
										video: true,
										audio: false
									},
									function(localMediaStream) {
										var video = document.querySelector('video.usermedia.demo');
										video.src = window.URL.createObjectURL(localMediaStream);
										video.play();
										setTimeout(function() {
											localMediaStream.stop();
											video.src = "";
										}, 5000);
									},
									function(err) {
										console.log("The following error occured: " + err);
									});
								} else {
									console.log("getUserMedia not supported");
								}
							});
						</script>
						<aside class="notes">
              No flash! We have acces to stream, frames, audio etc. Details later. Lots of potential.
						</aside>
					</section>

          <section data-state="subtheme-indigo">
            <h2><code>Web Audio API</code></h2>
            <p>
              <a class="audioapi demo">Demo</a>
            </p>
            <script>
                var context = new webkitAudioContext(); // Create audio container
                var oscillator;
                var playing = false;
                function toggle() {
                  if(playing) {
                    oscillator.stop();
                    playing = false;
                  } else {
                    oscillator = context.createOscillator(); // Create sound source
                    oscillator.connect(context.destination); // Connect sound to output
                    oscillator.noteOn(0);
                    playing = true;
                  }
                }
                document.querySelector('.audioapi.demo').addEventListener('click', toggle);
            </script>
            <aside class="notes">
              Access to decoders, generators, input and output devices, etc
            </aside>
          </section>
				</section>
				<section>
					<h2>Media Access?</h2>
					<p>
						<ul>
							<li>access media files (fragments)</li>
							<li>access media metadata</li>
							<li>access A/V devices</li>
							<li>generate and output audio</li>
							<li>process and record audio streams</li>
              <li>accessibility</li>
						</ul>
					</p>
					<aside class="notes">
					</aside>
				</section>
        <section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <p>
              Access fragments of media
            </p>
            <aside class="notes">
              “Completed Work”. Tight coupling with a draft “Fragment Resolution”.
              What are fragments? Fragments can be in temporal and spatial dimensions. Also tracks.
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Goal</h3>
            <p>
              <img src="assets/mediafragment.jpg">
            </p>
            <aside class="notes">
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h4>Dimensions</h4>
            <ul>
              <li>Spatial</li>
              <li>Temporal</li>
              <li>Tracks</li>
              <li>ID</li>
            </ul>
            <aside class="notes">
              ID is a a convenience layer for temporal dimension.
              Let’s start with the raw syntax. Where to apply it will
              be covered later.
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Temporal Fragmentation</h3>
            <p>
              Syntax:
              <pre><code>
t=&lt;start&gt;,&lt;end&gt;
              </code></pre>
            </p>
            <p>
              Example:
              <pre><code>
t=20,1:24:30
t=npt:50
t=,22:10
              </code></pre>
            </p>
            <aside class="notes">
              npt=Normal Play Time, could also be SMPTE timecodes (Society of Motion Picture & Television Engineering)
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Spatial Fragmentation</h3>
            <p>
              Syntax:
              <pre><code>
xywh=&lt;start&gt;,&lt;end&gt;
              </code></pre>
            </p>
            <p>
              Example:
              <pre><code>
xywh=160,120,320,240
xywh=pixel:160,120,320,240
xywh=percent:25,25,50,50
              </code></pre>
            </p>
            <aside class="notes">
              Both temporal and spatial fragmentation have exact definition how to validate and calculate the fragmented media. Where to apply the syntax? Let's talk about URIs.
            </aside>
          </section>
          <section data-state="subtheme-teal">
            <h2>URI</h2>
            <h3>Intermezzo</h3>
            <p>
              <pre><code>
  foo://example.com:8042/over/there?name=ferret#nose
  \_/   \______________/\_________/ \_________/ \__/
   |           |            |            |        |
scheme     authority       path        query   fragment
              </code></pre>
            </p>
            <aside class="notes">
              <p>
                URI as a abstraction of URLs. URLs by Tim Berners-Lee. URLs specify a location. Supposed to be persistent over time. Changing content via redirection.
              </p>
              <p>
                Fragment technically not part of resource locator (UserAgent handles it, will not be transferred to server).
              </p>
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Usage</h3>
            <p>
              Individual resource (server-side implementation)
              <pre><code class="highlighted">
http://yt.com/FL3MqSKLNHY<em>?t=20,30&xywh=160,120,320,240</em>
              </code></pre>
              Modifier on a resource (client-side implementation)
              <pre><code class="highlighted">
http://yt.com/FL3MqSKLNHY<em>#t=20,30&xywh=160,120,320,240</em>
              </code></pre>
            </p>
            <aside class="notes">
              Semantics! Technically no relation with server-side implementation. Reencoding.
              Standard defines how to validate and calculate fragment descriptions. video-Tag supports temporal only.
              How is server side implemented? How is client side implemented?
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Resolution</h3>
            <img src="assets/mediafragmentresolution.png">
            <aside class="notes">
              Original picture from the standard. Ugly. No SVG. I was fed up.
              Summary! When cached: easy. When not cached: Range request. Can the UA solve fragment2byte-range problem?
              When not: Special header or query.
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Augmented HTTP Range Header</h3>
            <pre><code data-hint="Request" class="highlighted">
GET /video.ogv HTTP/1.1
Host: www.example.com
Accept: video/*
<em>Range: t:npt=10-20</em>
            </code></pre>
            <pre><code data-hint="Response" class="highlighted">
HTTP/1.1 206 Partial Content
Accept-Ranges: bytes, t, id
Content-Length: 3743
Content-Type: video/ogg
<em>Content-Range: bytes 19147-22880/35614993
Content-Range-Mapping: { t:npt 9.85-21.16/0.0-653.79 }
  = { bytes 19147-22880/35614993 }</em>
Etag: "b7a60-21f7111-46f3219476580"

{binary data}
            </code></pre>
            <aside class="notes">
              What if client doesn’t have headers (resolution, color space, etc?)
            </aside>
          </section>
         <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Augmented HTTP Range Header</h3>
            <pre><code data-hint="Request" class="highlighted">
GET /video.ogv HTTP/1.1
Host: www.example.com
Accept: video/*
Range: t:npt=10-20<em>;include-setup</em>
            </code></pre>
            <pre><code data-hint="Response" class="highlighted">
HTTP/1.1 206 Partial Content
Accept-Ranges: bytes, t
Content-Length: 804020
<em>Content-Type: multipart/byteranges;boundary=End
Content-Range-Mapping:
  { t:npt 10.0-20.0/0-38.3;include-setup } =
  { bytes 0-1650,1264525-2066894/4055466 0}</em>
  --End
Content-Type: video/webm
Content-Range: bytes 0-1650/4055466
{binary data}
--End
Content-Type: video/webm
Content-Range: bytes 1264525-2066894/4055466
{binary data}
            </code></pre>
            <aside class="notes">

            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Media Fragments URI</h2>
            <h3>Support</h3>
            <figure>
              <img src="assets/mediafragmentsupport.png">
              <figcaption>
                <a href="http://caniuse.com/#search=fragments">caniuse.com/#search=fragments</a>
              </figcaption>
            </figure>
            <aside class="notes">
              Non-existent. caniuse.com by no means authorative, in this case symbolic.
            </aside>
          </section>
        </section>
        <section>
          <section data-state="subtheme-green">
            <h2>Metadata API</h2>
            <p>
              Metadata of media
            </p>
            <img src="assets/metadataexample.png">
            <aside class="notes">
              Examples of metadata, which currently can’t be read by webapps.
              Going to keep this short, boring and unusable.
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Metadata API</h2>
            <h3>Sources of metadata</h3>
            <ul>
              <li>User-Agent</li>
              <li>Media-Resource Web Service</li>
            </ul>

            <aside class="notes">
              Media identified by URI. User-Agent might inspect headers,
              web service can provide more data. Think IMDb.
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Metadata API</h2>
            <h3>Example</h3>
            <pre><code data-hint="JS">
mediaResource = new MediaResource();
aSyncObject = mediaResource.createMediaResource(
   "http://www.w3.org/.../MAWG-Stockholm-20090626.JPG",
   metadataSources, 1);
            </code></pre>
            <aside class="notes">
              Some JPG. What are those resources?
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Metadata API</h2>
            <h3>Example</h3>
            <pre><code data-hint="JS">
metadataSources = new MetadataSource[2];
metadataSources[0] = new MetadataSource(
    "http://www.w3.org/.../DC_example1.xml",
    "dc"
  );
metadataSources[1] = new MetadataSource(
    "http://www.w3.org/.../MAWG-Stockholm-20090626.JPG",
    "exif"
  );
            </code></pre>
            <aside class="notes">
              Unifying EXIF and another "Dublin Core".
            </aside>
          </section>
          <section data-state="subtheme-green">
            <h2>Metadata API</h2>
            <h3>Support</h3>
            <figure>
              <img src="assets/metadatasupport.png">
              <figcaption>This is Chrome Canary!</figcaption>
            </figure>
            <aside class="notes">
              Notice something? Both "completed works" are not supported at all.
            </aside>
          </section>
        </section>
        <section>
          <section data-state="subtheme-amber">
              <h2>HTML Media Capture</h2>
              <p>
                Capture media in forms with the <code>&lt;input&gt;</code> tag.
              </p>
              <aside class="notes">
                Short standard. Good standard. Unsupported standard. “Recommendation Candidate”.
              </aside>
          </section>
          <section data-state="subtheme-amber">
              <h2>HTML Media Capture</h2>
              <p>
                <pre><code data-hint="HTML" class="highlighted">
&lt;input type="file" <em>accept="image/*" capture</em>&gt;
&lt;input type="file" <em>accept="video/*" capture</em>&gt;
&lt;input type="file" <em>accept="audio/*" capture</em>&gt;
                </code></pre>
              </p>
              <aside class="notes">
                That’s it! In forms it will be multipart/mime but can also be accessed via JS.
              </aside>
          </section>
          <section data-state="subtheme-amber">
              <h2>HTML Media Capture</h2>
              <img src="assets/htmlinputpicker.png">
              <aside class="notes">
              </aside>
          </section>
          <section data-state="subtheme-amber">
              <h2>HTML Media Capture</h2>
              <h3>Support</h3>
              <input type="file" accept="image/*" capture>
              <aside class="notes">
                Nope. No support.
              </aside>
          </section>
        </section>
        <section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <p>
              Access to multimedia streams from local devices
            </p>
            <aside class="notes">
              Danger Zone! Draft!
              Typically, but not limited to, audio, video or both.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <img src="assets/mediastreamarchitecture.png">
            <aside class="notes">
              MediaTracks of a stream are synced.
            </aside>
          </section>
          <section data-state="subtheme-teal">
            <h2>WEBIDL</h2>
            <h3>Intermezzo</h3>
            <aside class="notes">
              Web Interface Definition Language. Self-explanatory.
              The Irony of an untyped langauge.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <pre><code data-hint="WEBIDL">
interface MediaStream : EventTarget {
    sequence&lt;MediaStreamTrack&gt; getAudioTracks ();
    sequence&lt;MediaStreamTrack&gt; getVideoTracks ();
    MediaStreamTrack?          getTrackById (DOMString trackId);
    void                       addTrack (MediaStreamTrack track);
    void                       removeTrack (MediaStreamTrack track);
    MediaStream                clone ();

    readonly    attribute DOMString    id;
    readonly    attribute boolean      ended;
                attribute EventHandler onended;
                attribute EventHandler onaddtrack;
                attribute EventHandler onremovetrack;
};
            </code></pre>
            <aside class="notes">
              clone() -> independent consumption, allows pipelines
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <pre><code data-hint="WEBIDL">
[Constructor,
  Constructor (MediaStream stream),
  Constructor (MediaStreamTrackSequence tracks)]
            </code></pre>
            <aside class="notes">
              Repackaging. Act as a source. Consider WebRTC.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <pre><code data-hint="WEBIDL">
interface NavigatorUserMedia {
    void getUserMedia (
      MediaStreamConstraints? constraints,
      NavigatorUserMediaSuccessCallback successCallback,
      NavigatorUserMediaErrorCallback errorCallback
    );
};
            </code></pre>
            <aside class="notes">
              Under discussion!
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <pre><code data-hint="JS" class="highlighted">
{
  <em>mandatory</em>: {
    width: { min: 640 },
    height: { min: 480 }
  },
  <em>optional</em>: [
    { width: 650 },
    { width: { min: 650 }},
    { frameRate: 60 },
    { width: { max: 800 }},
    { facingMode: "user" }
  ]
}
            </code></pre>
            <aside class="notes">
              Under discussion!
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <p>
              <pre><code data-hint="JS">
navigator.getUserMedia = (
  navigator.getUserMedia ||
  navigator.webkitGetUserMedia ||
  navigator.mozGetUserMedia ||
  navigator.msGetUserMedia
);
              </code></pre>
            </p>
            <aside class="notes">
              The usual cross-browser trickery
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <p>
              <pre><code data-hint="JS">
navigator.getUserMedia({
  video: true,
  audio: false
},
function(localMediaStream) {
  var video = document.querySelector('video');
  video.src = window.URL.createObjectURL(localMediaStream);
  video.play();
  setTimeout(function() {
    localMediaStream.stop();
    video.src = "";
  }, 5000);
},
function(err) {
  console.log("The following error occured: " + err);
});
              </code></pre>
            </p>
            <aside class="notes">
              This is the demo code.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Capture and Streams</h2>
            <h3>Support</h3>
            <img src="assets/mediastreamsupport.png">
          </section>
        </section>
        <section>
          <section data-state="subtheme-red">
            <h2>Media Recording</h2>
            <pre><code data-hint="WEBIDL">
[Constructor (MediaStream stream)]
interface MediaRecorder : EventTarget {
    readonly attribute MediaStream        stream;
    readonly attribute RecordingStateEnum state;
    void              record (optional long? timeslice);
    void              stop ();
    void              pause ();
    void              resume ();

    attribute EventHandler       ondataavailable;
    // ...
    // It’s a huge interface
    // ...
};
            </code></pre>
            <aside class="notes">
              Encodes into container/muxing (webm, maybe others). Returns blob (as in HTML5 blob).
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Media Recording</h2>
            <h3>Support</h3>
            Nope
          </section>
        </section>
        <section>
          <section data-state="subtheme-red">
            <h2>Image Capture</h2>
            <pre><code data-hint="WEBIDL">
[Constructor(VideoStreamTrack track)]
interface ImageCapture : EventTarget {
    readonly    attribute PhotoSettingsOptions photoSettingsOptions;
    readonly    attribute VideoStreamTrack     videoStreamTrack;
                attribute EventHandler         onphoto;
                attribute EventHandler         onerror;
                attribute EventHandler         onphotosettingschange;
                attribute EventHandler         onframegrab;
    void setOptions (PhotoSettings? photoSettings);
    void takePhoto ();
    void getFrame ();
};
            </code></pre>
            <aside class="notes">
              FrameGrab -> Return RGBA Canvas Thingy. Photo -> Apply settings (restart?), filters and return blob.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Image Capture</h2>
            <h3>Support</h3>
            <img src="assets/imagecapturesupport.png">
          </section>
        </section>
        <section>
          <section data-state="subtheme-red">
            <h2>Depth Stream</h2>
            <img src="assets/mediadepthstream.jpg">
          </section>
          <section data-state="subtheme-red">
            <h2>Depth Stream</h2>
            <pre><code data-hint="WEBIDL" class="highlighted">
<em>partial</em> dictionary MediaStreamConstraints {
    (boolean or MediaTrackConstraints) depth = false;
};            </code></pre>
            <pre><code data-hint="WEBIDL" class="highlighted">
<em>partial</em> interface MediaStream {
    sequence<MediaStreamTrack> getDepthTracks ();
};
            </code></pre>
          </section>
          <section data-state="subtheme-red">
            <h2>Depth Stream</h2>
            <h3>Support</h3>
            Nope
            <aside class="notes">
              Attention! Most depth-cameras (like Kinect) identify as normal
              cameras with b/w image stream.
            </aside>
          </section>
        </section>
        <section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Goals</h3>
            <ul>
              <li>Processing</li>
              <li>Synthesizing</li>
              <li>Audio Routing Graph Paradigm</li>
              <li>Sample-accurate sound playback with low latency</li>
              <li>LFOs, Envelopes, FFT, biquad filter, ...</li>
              <li>3D support</li>
            </ul>
            <aside class="notes">
              Think: MaxDSP/PureData. Pretty lon standard, but only because a lot of node types.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Bird’s view</h3>
            <figure>
              <img src="assets/webaudiorouting.png">
              <figcaption>“the right thing just happens”</figcaption>
            </figure>
            <aside class="notes">
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Audio Context</h3>
            <pre><code class="code" data-hint="WEBIDL">
[Constructor]
interface AudioContext : EventTarget {
    readonly attribute AudioDestinationNode destination;
    readonly attribute AudioListener listener;

    ScriptProcessorNode createScriptProcessor(...);

    AnalyserNode createAnalyser();
    GainNode createGain();
    DelayNode createDelay(optional double maxDelayTime = 1.0);
    BiquadFilterNode createBiquadFilter();
    // ...
};
            </code></pre>
            <aside class="notes">
              Creation of new nodes via context because initialization and low-level C++ engine.
              OfflineAudioContext for fast rendering.
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Audio Node</h3>
            <pre><code data-hint="WEBIDL">
interface AudioNode : EventTarget {
    void connect(
      AudioNode destination,
      optional unsigned long output = 0,
      optional unsigned long input = 0
    );
    void disconnect(optional unsigned long output = 0);

    readonly attribute AudioContext context;
    readonly attribute unsigned long numberOfInputs;
    readonly attribute unsigned long numberOfOutputs;

    attribute unsigned long channelCount;
    attribute ChannelCountMode channelCountMode;
    attribute ChannelInterpretation channelInterpretation;
};
            </code></pre>
          </section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Audio Listener</h3>
            <pre><code data-hint="WEBIDL">
interface AudioListener {

    attribute double dopplerFactor;
    attribute double speedOfSound;

    // Uses a 3D cartesian coordinate system
    void setPosition(double x, double y, double z);
    void setOrientation(double x, double y, double z, double xUp, double yUp, double zUp);
    void setVelocity(double x, double y, double z);

};
            </code></pre>
          </section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Oscillator Node</h3>
            <pre><code data-hint="WEBIDL">
interface OscillatorNode : AudioNode {

    attribute OscillatorType type;

    readonly attribute AudioParam frequency; // in Hertz
    readonly attribute AudioParam detune; // in Cents

    void start(double when);
    void stop(double when);
    void setPeriodicWave(PeriodicWave periodicWave);
};
            </code></pre>
            <small>Supports: "sine", "square", "sawtooth", "triangle", "custom"</small>
            <aside class="notes">
              Cent = 2^(1/1200)
            </aside>
          </section>
          <section data-state="subtheme-red">
            <h2>Web Audio API</h2>
            <h3>Support</h3>
            <img src="assets/webaudiosupport.png">
          </section>
        </section>
        <section data-state="subtheme-teal">
          <h1>Sources</h1>
          <ul>
            <li>http://www.w3.org/TR/2012/REC-media-frags-20120925/</li>
            <li>http://www.w3.org/TR/2011/WD-media-frags-recipes-20111201/</li>
            <li>http://ninsuna.elis.ugent.be/MediaFragmentsServer</li>
            <li>http://www.w3.org/TR/WebIDL/</li>
            <li>http://www.w3.org/TR/2013/WD-mediacapture-streams-20130903/</li>
            <li>http://www.w3.org/TR/2013/WD-mediastream-recording-20130205/</li>
            <li>http://www.w3.org/TR/2014/WD-mediacapture-depth-20141007/</li>
            <li>http://www.w3.org/TR/2013/WD-image-capture-20130709/</li>
            <li>http://www.w3.org/TR/2013/WD-webaudio-20131010/</li>
          </ul>
        </section>
        <section>
          <h1>Media Access</h1>
          <p>
            <small>by <a href="http://twitter.com/surmair" target="_blank">Alexander “Surma” Surma</a>, October 2014</small>
          </p>
          <p>
            <small><a href="http://surma-slides.github.io/mediaaccess">surma-slides.github.io/mediaaccess</a></small>
          </p>
        </section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>
			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: 'material-design', // available themes are in /css/theme
				transition: 'linear', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					// { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

			var baseTheme = Reveal.getConfig().theme;
			Reveal.addEventListener('slidechanged', function() {
				var c = document.documentElement.className;
				if(c.substr(0, 9) == 'subtheme-') {
					Reveal.configure({
						theme: baseTheme+'-'+c.substr(9)
					});
				} else {
					Reveal.configure({
						theme: baseTheme
					});
				}
			}, false );

		</script>

	</body>
</html>
